<!DOCTYPE html>
<html>
<head>
<title> 2024 Northeastern University London Essay Competition: Discuss the societal risks and rewards associated with generative AI </title>
<link rel="stylesheet" href="styles.css"> 
</head>

<body>
    <h1>2024 Northeastern University London Essay Competition: Discuss the societal risks and rewards associated with generative AI</h1>
    <p>
        With every nascent technological advancement, the fanfare surrounding it can be modelled as an S-shaped curve . A surge in popularity precedes an eventual plateau in demand, as evidenced by the fact that we no longer see people sleeping outside Apple for the latest iPhone. For the most part, no one knows where we are on the curve; is this the peak, and should we enjoy it as much as possible while it lasts? Or is this just the beginning of things to come? With the rise of Generative AI (GenAI), we face these same questions once again – perhaps the lack of uncertainty of which stage we are at is what drives its allure. What problems does GenAI solve? Equally, what problems does GenAI pose? This essay will explore both sides of the coin and take a deep dive into our future with GenAI.
    </p>
    <p>
         The alignment problem has been a mainstay in the minds of those working with AI for decades and its prevalence hasn’t abated. It can be defined as the divergence between a model’s capability and its tendency to achieve its expected results. With the rise of large-language models (LLMs) such as OpenAI’s ChatGPT which can output text that looks identical to what a human could’ve written, ensuring that the objectives and goals of GenAI are in tandem with the goals of humanity has now become increasingly difficult. Prior machine-learning systems were mainly faced with the task of ensuring that they didn’t fall into the hands of malicious actors who could use them unscrupulously – nowadays, with an innocuous prompt, image generators and chatbots are liable to output morally and ethically dubious content of what seems to be their own accord, despite this not being its designed goal. To counteract the threat of misaligned goals, programmers use a reward system to discourage unwanted behaviour, but this can cause GenAI to believe that the ends justify the means, whereby achievement of their goal takes priority over its methodology. Take a benign example, where a robot was given the simple goal of picking up a ball with a claw. Instead, it simply placed the claw between the user and the ball, appearing to be picking the ball up and achieving a desirable outcome (OpenAI 2017). This form of reward hacking is most seen in the form of hallucination – the goal of an LLM may be to produce human-like text, but in its search to maximise its reward, it overlooks the need to produce factually correct statements. 
    </p>
    <p>
        However, some could argue that the uncertainty over the veracity of GenAI’s responses, specifically LLMs, could be a benefit. Currently, GenAI has mainly been deployed in academia by students looking for help with their work, whether it be partial or complete assistance. Currently, it has shown that its responses are nowhere near perfect and should be used with caution. While this isn’t the intended goal, this could force users to ensure that they carry out prior research on the topic so they can corroborate the information provided, in case the chatbot hallucinates. This could prevent blind use of GenAI and discourage people from using it for complete aid, such as generating an entire essay, as it simply isn’t good or reliable enough to complete such a task. However, if GenAI does get to such a stage where it can comfortably complete intensive prompts, we could see a massive surge in creativity and scientific breakthroughs where students and workers can use GenAI as their personal assistant, trading new ideas and concepts and have it respond with equally skilled insights, allowing for greater developments and more advanced research. However, the risk of plagiarism remains pervasive - professors will need ways of ensuring that their students aren’t asking ChatGPT to wholly complete a project on their behalf. Consequently, methods of determining whether text is AI-generated or not are a necessity. Watermarks seemed to be an appropriate solution – simply embed a signal into the image or text produced which wouldn’t affect the response’s quality but be detected by an algorithm and show it was AI-generated. Yet, solutions have proved elusive as ChatGPT themselves will tell you as they had to take their own AI detector tool down due to its inaccuracy (Business Insider 2023). Furthermore, malicious actors could deliberately embed a watermark into original content to make it seem as if it were AI-generated to bring into question its authenticity, which could tarnish the reputation of authors and artists alike.
    </p>
    <p>
        Another proposed solution would be to develop restrictive AI - AI that is only to be used for very specific and niche jobs; by limiting its scope, perhaps we can limit the harm it causes. But this relies on the belief that AI is destined to reward hack, which may not be the case as robust reward functions can help limit the frequency of reward hacking. This then prevents us from using GenAI for larger, more complex problems where “simple” GenAI may be unsuitable. Furthermore, even if we attempt to specialise models of GenAI, malicious actors can still cause harm. One such method is through prompt injection, whereby a seemingly innocuous prompt has a hidden signal inside the message that causes the AI to respond to subsequent harmful questions. In this case, even if robust systems against explicitly dangerous prompts are made, these attacks are especially hard to prepare against as the noise encrypted in the message cannot be seen by humans but is recognised by the GenAI. A cruder attack vector is seen in the form of jailbreaking, where malicious actors attempt to subvert the restrictions placed on the GenAI by tricking it into responding to harmful prompts through a simple rewording of the question that isn’t detected by the GenAI. GenAI is also liable to become the number one source of deepfakes – as the quality of image and audio generators improves, the line between fiction and reality could rapidly blur as people are depicted to be in places they never were and carrying out actions they never committed.
    </p>
    <p>
        For companies incorporating GenAI as part of their workforce, unrefined outputs could be a major issue. Increasingly, software engineers are leveraging GenAI to produce lines of code that solve both simple and complex tasks (Harvard Business School - Digital Initiative 2022). However, if the repository of input data available to GenAI isn’t frequently updated, they run the security risk of using code that contains similar patterns – patterns that could be exploited by hackers. Additionally, the code produced by GenAI could be analogous to code written by someone else; if that source code has been copyrighted since after it was added as part of the AI’s training data, companies also run the risk of copyright infringement. This also applies to all training data supplied to LLMs: OpenAI has been hit with a lawsuit by the New York Times for “profiting from [..] massive copyright infringement” (Financial Times 2023). Furthermore, most systems using GenAI are based on a black-box model (Deloitte 2017), where we get “predictable results from an inexplicable mechanism” . Its methodology is indeterminable, which for some programmers may be a problem as the logic behind the solution may be more important than the actual solution itself. 
    </p>
    <p>
        Another risk posed by the introduction of GenAI into the workforce is its impact on the workers themselves. As with all automated processes, there is always a fear of whether it will cause the loss of people’s jobs; however, unlike the machines of the Industrial Revolution, it is not only “menial” workers who are at risk – people in technical sectors such as software engineering, consultancy, marketing could potentially see themselves out of a job as we see GenAI’s ability to carry out even more advanced tasks increase. The main worry is if a task can be automated, then the person whose role it was to carry out that task will no longer be needed. These fears of job replacement, however, rarely come to fruition. After the Industrial Revolution, the same worries were lobbied for jobs regarding processes that could be mechanised – yet for the weaving industry, one such sector, the number of jobs in the 18th century increased and so did their wages (Bessen 2015), and it is possible a similar thing could happen for jobs in which GenAI is involved in. An explanation for this would be that automation of processes allows for these tasks to be completed at a higher quality, which causes an increase in demand. Subsequently, more workers are required to meet these higher supply levels, therefore leading to more jobs. GenAI is far more likely to take over tasks, rather than take over jobs and these will probably be repetitive, mundane tasks which then frees up humans to focus on the more complex and challenging problems, thereby driving more innovation in sectors. For example, programmers using GitHub Copilot can now focus on the what rather than the how, and even if the outputs aren’t perfect, humans can consistently refine and improve the responses in an iterative feedback loop. In this scenario, job displacement rather than replacement is more likely to occur where people can upskill and find themselves in more senior roles.
    </p>
    <blockquote>
        “Dystopia is when robots do half of our jobs – utopia is when robots do half of our job.” 
    </blockquote>
    <p>
        In terms of the immediate risks to society, GenAI still has its clear limitations concerning the thoroughness and quality of its training data, the factual integrity of its outputs and our ability to verify its provenance. While these tools remain the most exciting breakthrough of this decade , we run the risk of allowing GenAI to be used in ways that contravene its potential and becoming misguided by its capabilities – as a result, it is imperative that we view GenAI as a “copilot.” It shouldn’t be the figurehead driving the operation, but rather an immensely powerful assistant that can be leveraged to complete simpler tasks, allowing us to focus on new advancements and breakthroughs.
    </p>
    <h3>
        Bibliography
    </h3>
    <p>
Bessen, James. 2015. Toil and Technology: Innovative technology is displacing workers to new jobs rather than replacing them entirely. 27 February. Accessed December 29, 2023. https://www.elibrary.imf.org/view/journals/022/0052/001/article-A007-en.xml.
Business Insider. 2023. ChatGPT's AI Detection Tool Taken Down Over Accuracy Concerns. 26 July2. Accessed December 29, 2023. https://www.businessinsider.com/openai-chatgpt-ai-detection-tool-shut-down-over-inaccuracy-2023-7?r=US&IR=T.
Deloitte. 2017. Black Box Intelligence: Future of Risk in the Digital Era | Deloitte US. 15 December. Accessed December 29, 2023. https://www2.deloitte.com/us/en/pages/advisory/articles/black-box-artificial-intelligence.html.
engineering.com. 2022. AI Successfully Competes with Humans in Coding Challenge. 15 December. Accessed December 29, 2023. https://www.engineering.com/story/ai-successfully-competes-with-humans-in-coding-challenge.
Financial Times. 2023. New York Times sues Microsoft and OpenAI in copyright case. 27 December. Accessed December 29, 2023. https://www.ft.com/content/23c15ce1-16c5-4b2f-804e-2c0da64e1972.
Harvard Business School - Digital Initiative. 2022. GitHub Copilot: An Indispensable game changer. 30 November. Accessed December 29, 2023. https://d3.harvard.edu/platform-digit/submission/github-copilot-an-indispensable-game-changer/.
Ngo, Chan, Mindermann. 2023. The Alignment Problem from a Deep Learning Perspective. 1 September. Accessed December 29, 2023. https://arxiv.org/pdf/2209.00626.pdf.
OpenAI. 2017. Learning from human preferences. 13 June. Accessed December 29, 2023. https://openai.com/research/learning-from-human-preferences.
Smith, Noah. 2022. American workers need lots and lots of robots. 5 September. Accessed December 29, 2023. https://www.noahpinion.blog/p/american-workers-need-lots-and-lots?utm_source=substack&utm_campaign=post_embed&utm_medium=web.
WIRED. 2023. Waluigi, Carl Jung, and the Case for Moral AI | WIRED. 25 May. Accessed December 29, 2023. https://www.wired.com/story/waluigi-effect-generative-artificial-intelligence-morality/.
    </p>
</body>

</html>